{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What is NLTK ?\n",
    "NLTK is a powerful Python package that provides a set of diverse natural languages algorithms.\n",
    "It is free, opensource, easy to use, large community, and well documented.\n",
    "NLTK consists of the most common algorithms such as \n",
    "    * tokenizing, \n",
    "    * part-of-speech tagging, \n",
    "    * stemming, sentiment analysis, \n",
    "    * topic segmentation, and \n",
    "    * named entity recognition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Basics of NLP for Text\n",
    "\n",
    "In this guide we are going to cover some topics that you need to familizrize yourself with while working with NLP\n",
    "\n",
    "1. Sentence Tokenization\n",
    "2. Word Tokenization\n",
    "3. Text  Lemmatization and Stemming\n",
    "4. Stop Words\n",
    "5. Parts Of Speech(POS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started.\n",
    "import the nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.TOKENIZATION\n",
    "\n",
    "* Tokenization is the first step in text analytics. \n",
    "* The process of breaking down a text paragraph into smaller chunks such as **words** or **sentence** is called Tokenization.\n",
    "* Token is a single entity that is building blocks for sentence or paragraph.\n",
    "\n",
    "* We can perform two types of tokenization.\n",
    "    1. Sententence Tokenization\n",
    "    2. Word Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Tokenization\n",
    "Sentence Tokenization \n",
    "* Sentence tokenizer breaks text paragraph into sentences.\n",
    "* It is also called sentence segmentation\n",
    "* The idea here looks very simple. In English and some other languages, we can split apart the sentences whenever we see a punctuation mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Mr. Smith, how are you doing today?',\n",
       " 'The weather is great, and city is awesome.The sky is pinkish-blue.',\n",
       " \"You shouldn't eat cardboard\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text_one = \"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.The sky is pinkish-blue. You shouldn't eat cardboard\"\n",
    "\n",
    "tokenized_text = sent_tokenize(text)\n",
    "\n",
    "# output\n",
    "tokenized_text\n",
    "\n",
    "# check the type of tokenized_text\n",
    "# type(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backgammon is one of the oldest known board games.\n",
      "Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East.\n",
      "It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll\n",
      "of two dice.\n"
     ]
    }
   ],
   "source": [
    "text_two = \"\"\"Backgammon is one of the oldest known board games. \n",
    "Its history can be traced back nearly 5,000 years to archeological discoveries in the Middle East. \n",
    "It is a two player game where each player has fifteen checkers which move between twenty-four points according to the roll\n",
    "of two dice.\n",
    "\"\"\"\n",
    "\n",
    "tokenized_text_two = sent_tokenize(text_two)\n",
    "\n",
    "tokenized_text_two\n",
    "\n",
    "# loop over the list \n",
    "for i in tokenized_text_two:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Word Tokenization\n",
    " * Word tokenizer breaks text paragraph into words.\n",
    " * also called word segmentation\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'is',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'city',\n",
       " 'is',\n",
       " 'awesome.The',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'pinkish-blue',\n",
       " '.',\n",
       " 'You',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'eat',\n",
       " 'cardboard']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_word = word_tokenize(text_one)\n",
    "tokenized_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backgammon\n",
      "is\n",
      "one\n",
      "of\n",
      "the\n",
      "oldest\n",
      "known\n",
      "board\n",
      "games\n",
      ".\n",
      "Its\n",
      "history\n",
      "can\n",
      "be\n",
      "traced\n",
      "back\n",
      "nearly\n",
      "5,000\n",
      "years\n",
      "to\n",
      "archeological\n",
      "discoveries\n",
      "in\n",
      "the\n",
      "Middle\n",
      "East\n",
      ".\n",
      "It\n",
      "is\n",
      "a\n",
      "two\n",
      "player\n",
      "game\n",
      "where\n",
      "each\n",
      "player\n",
      "has\n",
      "fifteen\n",
      "checkers\n",
      "which\n",
      "move\n",
      "between\n",
      "twenty-four\n",
      "points\n",
      "according\n",
      "to\n",
      "the\n",
      "roll\n",
      "of\n",
      "two\n",
      "dice\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "tokenized_word_two = word_tokenize(text_two)\n",
    "\n",
    "for i in tokenized_word_two:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.TEXT LEMMATIZATION AND STEMMING\n",
    "\n",
    "For grammatical reasons, documents can contain different forms of a word such as drive, drives, driving. Also, sometimes we have related words with a similar meaning, such as nation, national, nationality.\n",
    "\n",
    "#### Why do we perform text lemmatization and stemming?\n",
    "The goal of both stemming and lemmatization is to reduce inflectional(involving a change in the form of a word to express a grammatical function or attribute.) forms and sometimes derivationally related forms of a word to a common base form.\n",
    "\n",
    "Examples:\n",
    "    * am, are, is => be\n",
    "    * dog, dogs, dog’s, dogs’ => dog\n",
    "    \n",
    "The result of this mapping applied on a text will be something like that:\n",
    "\n",
    "    * the boy’s dogs are different sizes => the boy dog be differ size\n",
    "    \n",
    "Stemming and lemmatization are special cases of **normalization**. However, they are different from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "* Stemming is the process of obtaining the root word from the word given.\n",
    "* Using efficient and well-generalized rules, all tokens can be cut down to obtain the root word, also known as the *stem*.\n",
    "* Stemming is a purely rule-based process through which we club together variations of the token.\n",
    "\n",
    "a basic rule-based stemmer, like removing –s/es or -ing or -ed can give you a precision(accuracy) of more than 70 percent .\n",
    "\n",
    "Example1\n",
    "\n",
    "The word *sit* can have different variations like \n",
    "1. sitting\n",
    "2. sat\n",
    "\n",
    "Example 2\n",
    "\n",
    "The word *connect* can have different variations like\n",
    "1. connection\n",
    "2. connected\n",
    "3. connecting\n",
    "\n",
    "\n",
    "**NB:\n",
    "a stemmer operates without knowledge of the context, and therefore cannot understand the difference between words which have different meaning depending on part of speech.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meet\n",
      "connect\n",
      "goe\n",
      "regist\n",
      "better\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "word1 = \"meeting\"\n",
    "word2 = \"connecting\"\n",
    "word3 = \"goes\"\n",
    "word4 = \"registered\"\n",
    "word5 = \"better\"\n",
    "\n",
    "stem_word1 = ps.stem(word1)\n",
    "stem_word2 = ps.stem(word2)\n",
    "stem_word3 = ps.stem(word3)\n",
    "stem_word4 = ps.stem(word4)\n",
    "stem_word5 = ps.stem(word5)\n",
    "\n",
    "print(stem_word1)\n",
    "print(stem_word2)\n",
    "print(stem_word3)\n",
    "print(stem_word4)\n",
    "print(stem_word5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "* Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words. \n",
    "* normally the aim is  to remove inflectional endings only and to return the base or dictionary form of a word,\n",
    "* This is known as the **lemma**.\n",
    "*  Lemmatization makes use of the context and POS tag to determine the inflected form(shortened version) of the word and various normalization rules are applied for each POS tag to get the root word (lemma).\n",
    "\n",
    "Examples:\n",
    "\n",
    "The word “better” has “good” as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\n",
    "\n",
    "The word “play” is the base form for the word “playing”, and hence this is matched in both stemming and lemmatization.\n",
    "\n",
    "The word “meeting” can be either the base form of a noun or a form of a verb (“to meet”) depending on the context; e.g., “in our last meeting” or “We are meeting again tomorrow”. Unlike stemming, lemmatization attempts to select the correct lemma depending on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly\n",
      "drive\n",
      "registered\n",
      "register\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "word1 = \"flying\"\n",
    "word2 = \"drove\"\n",
    "word3 = \"better\"\n",
    "word3 = \"registered\"\n",
    "\n",
    "lem_word1 = lem.lemmatize(word1,\"v\") #wordnet v-verb\n",
    "lem_word2 = lem.lemmatize(word2,\"v\")\n",
    "lem_word3 = lem.lemmatize(word3,\"a\")\n",
    "lem_word4 = lem.lemmatize(word4,\"v\")\n",
    "\n",
    "print(lem_word1)\n",
    "print(lem_word2)\n",
    "print(lem_word3)\n",
    "print(lem_word4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.STOPWORDS\n",
    "\n",
    "* Stop words usually refer to the most common words such as “and”, “the”, “a” in a language, but there is no single universal list of stopwords.\n",
    "* Text may contain stop words such as is, am, are, this, a, an, the, etc.\n",
    "* **Stopwords** act as bridges and their job is to ensure that sentences are grammatically correct.\n",
    "* Thus, removing the words that occur commonly in the corpus is the definition of stop-word removal\n",
    "\n",
    "Stop words are filtered out before or after processing of text. When applying machine learning to text, these words can add a lot of noise. That’s why we want to remove these irrelevant words.\n",
    "\n",
    "The NLTK tool has a predefined list of stopwords that refers to the most common words. If you use it for your first time, you need to download the stop words using this code: nltk.download(“stopwords”).\n",
    "\n",
    "Once we complete the downloading, we can load the stopwords package from the nltk.corpus and use it to load the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Backgammon', 'one', 'oldest', 'known', 'board', 'games', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "sentence = \"Backgammon is one of the oldest known board games.\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "without_stop_words = [word for word in words if not word in stop_words]\n",
    "print(without_stop_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Part-of-Speech(POS) TAGGING\n",
    "\n",
    "* The primary target of Part-of-Speech(POS) tagging is to identify the grammatical group of a given word.\n",
    "* Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context.\n",
    "* POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albert', 'Einstein', 'was', 'born', 'in', 'Ulm', ',', 'Germany', 'in', '1879', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Albert', 'NNP'),\n",
       " ('Einstein', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('born', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('Ulm', 'NNP'),\n",
       " (',', ','),\n",
       " ('Germany', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('1879', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Albert Einstein was born in Ulm, Germany in 1879.\"\n",
    "\n",
    "tokens=nltk.word_tokenize(sent)\n",
    "print(tokens)\n",
    "\n",
    "nltk.pos_tag(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
